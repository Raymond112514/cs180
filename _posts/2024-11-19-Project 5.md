---
layout: post
---

## Part A: The Power of Diffusion Models!

### A.1 Forward Process

Before diving into diffusion models, we first focus on the problem of denoisingâ€”removing noise from noisy images. The data we will use consists of a collection of noisy images at different scales. The noisy image is defined as:  

$$x_{noised} = x + \sigma \epsilon$$  

where $x$ denote the clean image and $\epsilon \sim N(0, 1)$.  

We show a few examples of noisy images at scales $250$, $500$, and $700$, respectively, in the figure below.  

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_1.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Noised images of different scale.</p>

### A.2 Classical Denoising

We now attempt to denoise the images. The classical approach to denoising is to apply a Gaussian filter, as shown below.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_2.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 2:</strong> Gaussian denoising (Top row) Noisy image (Bottom row) Gaussian denoised images.</p>

Note that the Gaussian filter is not sufficient to completely remove the noise.

### A.3 One-Step Denoising

We now use a pretrained diffusion model for denoising. Specifically, we utilize the DeepFloyd IF diffusion model, which supports text prompts. Below, we showcase some examples of DeepFloyd IF generations along with their corresponding prompts.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/sample.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 3:</strong> Sample generation of DeepFloyd model. </p>

Using the UNet architecture from the denoiser, we apply single-step denoising. The results are presented below.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_3.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 4:</strong> One step denoised images (Top) Noisy image (Bottom) One-step denoised image. For the original image, see Figure 6. </p>

Note that the denoised results are significantly better compared to Gaussian denoising.

### A.4 Iterative Denoising

To further improve the performance of denoising, we can increase the number of steps, leading to an iterative denoising method. To simplify the calculations, we used strided timesteps. The noised images are shown below.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_4_traj.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 5:</strong> Noisy images.</p>

Comparing the results of iterative denoising with other methods, we observe that iterative denoising provides the best results with the finest details.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_4_compare.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 6:</strong> Comparison of different methods.</p>

### A.5 Diffusion Model Sampling

In the previous section, we used the diffusion model for denoising. However, we can also use the iterative denoising method to generate images. By starting with a random image and progressively denoising, we can achieve the desired results. Some examples are shown below, using the prompt set to `"a high-quality photo."`

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_5.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 7:</strong> Sample generated images using the diffusion model.</p>

### A.6 Classifier-Free Guidance (CFG)

We can enhance the quality of the images through classifier-free guidance, which uses both conditional and unconditional noise estimates to represent the noise. The noise $\epsilon$ is given by  

$$\epsilon = \epsilon_u + \gamma(\epsilon_c - \epsilon_u)$$  

where $\gamma$ is a parameter that controls the strength of guidance. Below are some examples of images generated using classifier-free guidance.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_6.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 8:</strong> Sample generated images using classifier free guidance.</p>

### A.7 Image-to-image Translation

#### SDE Edit

We now move on to different applications of image generation. The first application is image editing, which involves slightly noising the image and then performing denoising. Below are the results of applying different noise levels to the test image.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_7_1.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 9:</strong> SDEdit on test image.</p>

Note that the images gradually resemble the original images. We can also apply the same method to images of our choice, including web images and hand-drawn images.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_7_2.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 10:</strong> More SDEdit examples.</p>

#### Inpainting

Another similar application is inpainting. By removing certain regions of an image and filling them with noise, we can generate new parts for the image. Below are some examples using the test image and a few internet images.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_7_3.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 11:</strong> Inpainting image examples.</p>

#### Text-Conditional Image-to-Image Translation

During the process of reverse diffusion, we can also guide the process using different prompts. Below are some examples with different prompts. The prompts for the three rows are `"a rocket ship"`, `"a pencil"`, and `"a photo of a dog"`, respectively.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_7_4.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 12:</strong> Text conditional generation examples.</p>

### A.8 Visual Anagrams

Diffusion models can also be used to create optical illusions. We allowed the approach of Visual Anagrams: Generating Multi-View
Optical Illusions with Diffusion Models to generate anagrams. The idea is that we input two prompts, which is used to make two different noise estimates

$$\epsilon_1=\epsilon_\theta(x_t, t, p_1)\hspace{5mm}\epsilon_2=\text{flip}(\epsilon_\theta(\text{flip}(x_t), t, p_2))$$

The final noise estimate will then become the average of these two. Below shows some reuslt of visual anagrams. 

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_8.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 13:</strong> Generated visual anagrams.</p>

### A.9 Hybrid Images

We can also create hybrid images. Similar to before, we take in two prompts, $p_1$ and $p_2$. The noise estimates for the prompts are:

$$\epsilon_1 = \epsilon_\theta(x_t, t, p_1) \hspace{5mm} \epsilon_2 = \epsilon_\theta(x_t, t, p_2)$$

The final noise estimate is then given by:

$$\epsilon = f_{low \, pass}(\epsilon_1) + f_{high \, pass}(\epsilon_2)$$

The rest of the process follows the same procedure. Some examples of generated hybrid images are shown below.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/1_9.png" alt="Image 1" style="width: 100%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 14:</strong> Generated hybrid images.</p>

## Part B: Diffusion Models

### Denoiser Model

We now implement diffusion model froms scratch. Before implementing the diffusion model, we start by implementing a denoiser, which takes in a noised images and tries to remove the noise present in the image. To do so, we used the U-Net architecture as follows. The dataset is MNIST that has benn corrupted with additive noise, as follows

$$z_i=x_i+\sigma \epsilon_i$$

We show the impact of different noise in the figure below.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/noise_visualization.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

The network, during training, takes in a noised image and tries to output the desnoised image, the stochastic loss is then defined by 

$$ L =\frac{1}{n}\sum_{i=1}^n ||D_{\theta}(x_i+\sigma\epsilon_i) - x_i||_2^2$$

For the purpose of our training, we set $\sigma=0.5$. The network is trained for $5$ epochs with Adam optimizer with learning rate $10^{-4}$ and batch size of $32$. The training loss curve is shown below. 

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/denoiser_loss_curve.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

We dispplay the denoising result after the 1st and 5th epochs as below.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/epoch_one_result_denoise.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/epoch_five_result_denoise.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

We see that the model successfully removed the noises and recovered the handwritten images. We can also test the models' performance on different noise level. From the figure below, we can see that our model is qwuite robust at different noise levels, despite not being trained on them.

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/diff_schedule.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

### Diffusion Model

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/diffusion_loss_curve.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/diffusion_visual.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

### Diffusion Model With Class Conditioning

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/class_diffusion_loss.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>

<div style="display: flex; justify-content: center;">   
   <img src="{{ site.baseurl }}/assets/proj5_images/class_diffusion_visual.png" alt="Image 1" style="width: 90%; height: auto;"> 
</div> 
<p style="text-align: center; margin-top: 15px;"><strong>Figure 1:</strong> Labeled source and target images.</p>





